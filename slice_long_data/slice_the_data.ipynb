{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8DOVRrOG3vM"
      },
      "source": [
        "# Install Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMe-Qemdtzs8",
        "outputId": "c8286d80-d07b-4ce6-c7c5-0f7da79eac2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi8vdj0ZG0ig",
        "outputId": "d99f1c7a-3a0e-4a32-9c9c-4db433f63a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.2.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=0240555a8623188a5633d4f44eb8b4a5e14b25907333d547e8424046bccd9856\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!apt install -y ffmpeg\n",
        "!pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-2XjTLG8fNXG"
      },
      "outputs": [],
      "source": [
        "# copy the data to the local directory\n",
        "!cp -r /content/drive/My\\ Drive/files\\ for\\ slice\\ long\\ samples/* /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-_v5N5WahaG"
      },
      "source": [
        "# parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-pZfeePlag1u"
      },
      "outputs": [],
      "source": [
        "WITH_TEAMIM = False\n",
        "WITH_STRESS = False\n",
        "SR = 16000\n",
        "SLICE_TO_WAV = False\n",
        "\n",
        "if WITH_TEAMIM and WITH_STRESS:\n",
        "    print(\"WITH_TEAMIM and WITH_STRESS cannot be True at the same time.\")\n",
        "    exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV_pQ_0-HEMb"
      },
      "source": [
        "# Get WhisperTimeSync"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNNMTmGMHH0J",
        "outputId": "833c5713-bb36-44dd-ee83-48f44a5e2be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'WhisperTimeSync'...\n",
            "remote: Enumerating objects: 219, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 219 (delta 44), reused 46 (delta 38), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (219/219), 2.74 MiB | 25.74 MiB/s, done.\n",
            "Resolving deltas: 100% (102/102), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf WhisperTimeSync\n",
        "!git clone https://github.com/EtienneAb3d/WhisperTimeSync.git\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRAaptJQHr_y"
      },
      "source": [
        "# Transcribe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywI49qUPaTF2",
        "outputId": "e681a1dd-29af-4005-e33d-729d75ca4583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import json\n",
        "import io\n",
        "from six.moves.urllib.request import urlopen\n",
        "import pathlib\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "from nikud_and_teamim import remove_nikud, replace_teamim_with_emphasis, remove_nikud_and_teamim\n",
        "import re\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zm3ojmClrsfs"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/texts_without_cantillations\n",
        "!unzip -q /content/texts_without_cantillations.zip -d /content/texts_without_cantillations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0eKoiw2gujri"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import requests\n",
        "import json\n",
        "import scipy.io.wavfile as wavfile\n",
        "import io\n",
        "from six.moves.urllib.request import urlopen\n",
        "import pathlib\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9FgOXP8aTF4",
        "outputId": "df52700d-9c94-4a47-e775-37b892504a99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 16%|█▌        | 30/187 [01:49<09:34,  3.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Whisper model: large-v3 ...\n",
            " 31%|███████████▋                          | 906M/2.88G [00:35<00:47, 45.0MiB/s]"
          ]
        }
      ],
      "source": [
        "SR = 16000\n",
        "\n",
        "\n",
        "books_data = [\n",
        "    {\"name\": \"Bereshit\", \"number\": 1, \"chapters\": 50, \"sefaria_name\": \"Genesis\"},\n",
        "    {\"name\": \"Shemot\", \"number\": 2, \"chapters\": 40, \"sefaria_name\": \"Exodus\"},\n",
        "    {\"name\": \"Vaikra\", \"number\": 3, \"chapters\": 27, \"sefaria_name\": \"Leviticus\"},\n",
        "    {\"name\": \"Bamidbar\", \"number\": 4, \"chapters\": 36, \"sefaria_name\": \"Numbers\"},\n",
        "    {\"name\": \"Dvarim\", \"number\": 5, \"chapters\": 34, \"sefaria_name\": \"Deuteronomy\"}\n",
        "]\n",
        "sefer_names = [\"\", \"בְּרֵאשִֽׁית\", \"שְׁמֽות\", \"וַיִּקְרָֽא\", \"בַּמִּדְבָּֽר\", \"דְּבָרִֽים\"]\n",
        "otiyot_gematria = [\"\", \"אָֽלֶף\", \"בֵּֽית\", \"גִּֽימֶל\", \"דָּֽלֶת\", \"הֵֽא\", \"וָֽו\", \"זַֽיִן\", \"חֵֽית\", \"טֵֽית\", \"יֽוֹד\", \"יֽוֹד-אָֽלֶף\", \"יֽוֹד-בֵּֽית\", \"יֽוֹד-גִּֽימֶל\", \"יֽוֹד-דָּֽלֶת\", \"טֵֽית-וָֽו\", \"טֵֽית-זַֽיִן\", \"יֽ-זַֽיִן\", \"יֽוֹד-חֵֽית\", \"יֽוֹד-טֵֽית\", \"כַּֽף\", \"כַּֽף-אָֽלֶף\", \"כַּֽף-בֵּֽית\", \"כַּֽף-גִּֽימֶל\", \"כַּֽף-דָּֽלֶת\", \"כַּֽף-הֵֽא\", \"כַּֽף-וָֽו\", \"כַּֽף-זַֽיִן\", \"כַּֽף-חֵֽית\", \"כַּֽף-טֵֽית\", \"לָֽמֶד\", \"לָֽמֶד-אָֽלֶף\", \"לָֽמֶד-בֵּֽית\", \"לָֽמֶד-גִּֽימֶל\", \"לָֽמֶד-דָּֽלֶת\", \"לָֽמֶד-הֵֽא\", \"לָֽמֶד-וָֽו\", \"לָֽמֶד-זַֽיִן\", \"לָֽמֶד-חֵֽית\", \"לָֽמֶד-טֵֽית\", \"מֵֽם\", \"מֵֽם-אָֽלֶף\", \"מֵֽם-בֵּֽית\", \"מֵֽם-גִּֽימֶל\", \"מֵֽם-דָּֽלֶת\", \"מֵֽם-הֵֽא\", \"מֵֽם-וָֽו\", \"מֵֽם-זַֽיִן\", \"מֵֽם-חֵֽית\", \"מֵֽם-טֵֽית\", \"נֽוּן\", \"נֽוּן-אָֽלֶף\", \"נֽוּן-בֵּֽית\", \"נֽוּן-גִּֽימֶל\", \"נֽוּן-דָּֽלֶת\", \"נֽוּן-הֵֽא\", \"נֽוּן-וָֽו\", \"נֽוּן-זַֽיִן\", \"נֽוּן-חֵֽית\", \"נֽוּן-טֵֽית\", \"סָֽמֶךְ\"]\n",
        "otiyot_gematria = [remove_nikud(ot) for ot in otiyot_gematria]\n",
        "dataset = {\"text\": [], \"audio_file\": []}\n",
        "\n",
        "with open('links_for_audio_1.json', 'r') as f:\n",
        "    links_for_audio = json.load(f)\n",
        "\n",
        "# Calculate total number of chapters\n",
        "total_chapters = sum(book[\"chapters\"] for book in books_data)\n",
        "\n",
        "error_chapters = []\n",
        "\n",
        "# Create a progress bar\n",
        "pbar = tqdm(total=total_chapters)\n",
        "\n",
        "# Loop over all the chapters of all the books\n",
        "for book in books_data:\n",
        "    book_name = book[\"name\"]\n",
        "    for chapter in range(1, book[\"chapters\"]+1):\n",
        "        # check if the chapter already exists\n",
        "        if pathlib.Path(f\"/content/drive/MyDrive/cantilationless_audio/{book_name}_{chapter}.mp3\").exists():\n",
        "            pbar.update()\n",
        "            continue\n",
        "        # # test on small data\n",
        "        # if chapter % 27 != 0: # test on small data\n",
        "        #     continue\n",
        "        url = links_for_audio[\"books\"][book[\"number\"]-1][\"chapters\"][chapter-1][\"link\"]\n",
        "        # if get error in urlopen(url).read() we will add the chapter to error_chapters\n",
        "        try:\n",
        "            file = urlopen(url).read()\n",
        "        except:\n",
        "            error_chapters.append((book_name, chapter))\n",
        "            print(f\"All the chapters that we get error in urlopen(url).read() are: {error_chapters}\")\n",
        "            pbar.update()\n",
        "            continue\n",
        "        z = io.BytesIO(file)\n",
        "        pathlib.Path((f\"{book_name}_{chapter}.mp3\")).write_bytes(z.getbuffer())\n",
        "        audio, sr = librosa.load(f\"{book_name}_{chapter}.mp3\", sr=SR)\n",
        "        # Get the text of the chapter from file\n",
        "        with open(f\"texts_without_cantillations/{book['name']}.{chapter}.txt\", \"r\") as f:\n",
        "            text = f.read()\n",
        "        # Remove text in () and []:\n",
        "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "        text = re.sub(r'\\[[^)]*\\]', '', text)\n",
        "        if WITH_TEAMIM:\n",
        "            text = remove_nikud(text)\n",
        "        else:\n",
        "            if WITH_STRESS:\n",
        "                text = remove_nikud(text)\n",
        "                text = replace_teamim_with_emphasis(text)\n",
        "            else:\n",
        "                text = remove_nikud_and_teamim(text)\n",
        "\n",
        "        text = f\"{sefer_names[book['number']]} \" + f\"פרק {otiyot_gematria[chapter]} \" + text\n",
        "        with open(f\"{book_name}_{chapter}.txt\", \"w\") as f:\n",
        "            f.write(text)\n",
        "        # Transcribe the audio to get the timestamps (as srt file)\n",
        "        !python3 WhisperTimeSync/transcribe.py /content/\"{book_name}_{chapter}.mp3\" large-v3\n",
        "        with open(f\"{book_name}_{chapter}.srt\", \"r\") as f:\n",
        "            transcribed = f.read()\n",
        "        if \"תודה רבה\" in transcribed:\n",
        "            print(\"we get the error of 'תודה רבה', we will try with v2 model\")\n",
        "            # Remove the srt file\n",
        "            !rm \"{book_name}_{chapter}.srt\"\n",
        "            !python3 WhisperTimeSync/transcribe.py /content/\"{book_name}_{chapter}.mp3\" large-v2\n",
        "            with open(f\"{book_name}_{chapter}.srt\", \"r\") as f:\n",
        "                transcribed = f.read()\n",
        "            if \"תודה רבה\" in transcribed:\n",
        "                print(\"!!!!! we get the error of 'תודה רבה', also with v2 model !!!!!!!!\")\n",
        "                !rm \"{book_name}_{chapter}.srt\"\n",
        "                print(\"we will try with v1 model\")\n",
        "                !python3 WhisperTimeSync/transcribe.py /content/\"{book_name}_{chapter}.mp3\" large\n",
        "                with open(f\"{book_name}_{chapter}.srt\", \"r\") as f:\n",
        "                    transcribed = f.read()\n",
        "\n",
        "        # Align(sync) the text with the audio\n",
        "        !java -Xmx2G -jar WhisperTimeSync/distrib/WhisperTimeSync.jar /content/\"{book_name}_{chapter}.srt\" /content/\"{book_name}_{chapter}.txt\" he\n",
        "        with open(f\"{book_name}_{chapter}.txt.srt\", \"r\") as f:\n",
        "            srt = f.read()\n",
        "        if SLICE_TO_WAV:\n",
        "            for i, line in enumerate(srt.split(\"\\n\")):\n",
        "                if i % 4 == 0: # The line is a number of the subtitle\n",
        "                    continue\n",
        "                if i % 4 == 1: # The line is a time\n",
        "                    start, end = line.split(\" --> \")\n",
        "                    start = start.replace(\",\", \".\")\n",
        "                    end = end.replace(\",\", \".\")\n",
        "                    start = start.split(\":\")\n",
        "                    end = end.split(\":\")\n",
        "                    start = float(start[0])*3600 + float(start[1])*60 + float(start[2])\n",
        "                    end = float(end[0])*3600 + float(end[1])*60 + float(end[2])\n",
        "                    start = float(start)\n",
        "                    end = float(end)\n",
        "                    continue\n",
        "                if i % 4 == 2: # The line is the text\n",
        "                    dataset[\"text\"].append(line)\n",
        "                    dataset[\"audio_file\"].append(f\"{book_name}_{chapter}_{int(i/4):02}.wav\")\n",
        "                    # Save the audio file\n",
        "                    sf.write(f\"{book_name}_{chapter}_{int(i/4):02}.wav\", audio[int(start*SR):int(end*SR)], SR)\n",
        "                if i % 4 == 3: # The line is an empty line\n",
        "                    continue\n",
        "            # Remove the original audio file and the srt file\n",
        "            !rm \"{book_name}_{chapter}.txt.srt\"\n",
        "            !rm \"{book_name}_{chapter}.mp3\"\n",
        "\n",
        "        # Remove the text file and the srt(low quality subtitles) file\n",
        "        !rm \"{book_name}_{chapter}.srt\"\n",
        "        !rm \"{book_name}_{chapter}.txt\"\n",
        "\n",
        "        # Move the srt file and the audio file to the correct folder\n",
        "        !mv \"{book_name}_{chapter}.txt.srt\" /content/drive/MyDrive/cantilationless_srt/\"{book_name}_{chapter}\".srt\n",
        "        !mv \"{book_name}_{chapter}.mp3\" /content/drive/MyDrive/cantilationless_audio/\"{book_name}_{chapter}\".mp3\n",
        "\n",
        "\n",
        "        # Update the progress bar\n",
        "        pbar.update()\n",
        "\n",
        "# Save the dataset\n",
        "with open(\"dataset.json\", \"w\") as f:\n",
        "    json.dump(dataset, f)\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGhMQzDtGcbw"
      },
      "outputs": [],
      "source": [
        "\n",
        "book_name = \"Bereshit\"\n",
        "chapter = 27\n",
        "url = links_for_audio[\"books\"][1][\"chapters\"][chapter-1][\"link\"]\n",
        "z = io.BytesIO(urlopen(url).read())\n",
        "pathlib.Path((f\"{book_name}_{chapter}.mp3\")).write_bytes(z.getbuffer())\n",
        "!python3 WhisperTimeSync/transcribe.py /content/\"{book_name}_{chapter}.mp3\" large-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvWsPC_r7jwb"
      },
      "outputs": [],
      "source": [
        "# Save the dataset\n",
        "with open(\"dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9C7gVyYq8epF",
        "outputId": "b57cac95-52bc-4bd7-ac73-d6605b308f0e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-67430199f127>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mipd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mipd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# play random audio and print its text\n",
        "import random\n",
        "import IPython.display as ipd\n",
        "index = random.randint(0, len(dataset[\"audio_file\"]))\n",
        "print(dataset[\"text\"][index])\n",
        "ipd.Audio(dataset[\"audio_file\"][index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTl8-vmy9Rvb"
      },
      "outputs": [],
      "source": [
        "index = 14\n",
        "print(dataset[\"text\"][index])\n",
        "ipd.Audio(dataset[\"audio_file\"][index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttz8sp5caTF8",
        "outputId": "36db8b37-d4cc-449b-8397-5d5d6c96818b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "java.io.FileNotFoundException: {book_name}_{chapter}.srt (No such file or directory)\n",
            "\tat java.base/java.io.FileInputStream.open0(Native Method)\n",
            "\tat java.base/java.io.FileInputStream.open(FileInputStream.java:219)\n",
            "\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n",
            "\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:112)\n",
            "\tat com.cubAIx.WhisperTimeSync.WhisperTimeSync.load(WhisperTimeSync.java:20)\n",
            "\tat com.cubAIx.WhisperTimeSync.WhisperTimeSync.processFile(WhisperTimeSync.java:45)\n",
            "\tat com.cubAIx.WhisperTimeSync.WhisperTimeSync.main(WhisperTimeSync.java:120)\n"
          ]
        }
      ],
      "source": [
        "# Sync the times of the real text using the srt file\n",
        "!java -Xmx2G -jar WhisperTimeSync/distrib/WhisperTimeSync.jar \"{book_name}_{chapter}.srt\" \"{book_name}_{chapter}.txt\" he"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T_K00vTREqF",
        "outputId": "3bf9f37f-f759-4ee1-92d2-0a297c2ff923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat: /content/51.Shmot_1.srt: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cat /content/51.Shmot_1.srt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvOQ8eiSPRWt"
      },
      "source": [
        "# Synchronize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMJLFqrzPVjd"
      },
      "outputs": [],
      "source": [
        "!java -Xmx2G -jar WhisperTimeSync/distrib/WhisperTimeSync.jar /content/51.Shmot_1.srt /content/Exodus.1.txt he"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTc53xX5QpAR"
      },
      "outputs": [],
      "source": [
        "!cat /content/Exodus.1.txt.srt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
